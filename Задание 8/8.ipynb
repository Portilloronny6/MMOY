{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Using the toolkit of the datasets module, import the built-in iris dataset\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%# # display the names of the keys of the set. # display the names of the keys of the set.\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the names of the keys of the set.\n",
    "iris = load_iris()\n",
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('.. _iris_dataset:\\n'\n",
      " '\\n'\n",
      " 'Iris plants dataset\\n'\n",
      " '--------------------\\n'\n",
      " '\\n'\n",
      " '**Data Set Characteristics:**\\n'\n",
      " '\\n'\n",
      " '    :Number of Instances: 150 (50 in each of three classes)\\n'\n",
      " '    :Number of Attributes: 4 numeric, predictive attributes and the class\\n'\n",
      " '    :Attribute Information:\\n'\n",
      " '        - sepal length in cm\\n'\n",
      " '        - sepal width in cm\\n'\n",
      " '        - petal length in cm\\n'\n",
      " '        - petal width in cm\\n'\n",
      " '        - class:\\n'\n",
      " '                - Iris-Setosa\\n'\n",
      " '                - Iris-Versicolour\\n'\n",
      " '                - Iris-Virginica\\n'\n",
      " '                \\n'\n",
      " '    :Summary Statistics:\\n'\n",
      " '\\n'\n",
      " '    ============== ==== ==== ======= ===== ====================\\n'\n",
      " '                    Min  Max   Mean    SD   Class Correlation\\n'\n",
      " '    ============== ==== ==== ======= ===== ====================\\n'\n",
      " '    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n'\n",
      " '    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n'\n",
      " '    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n'\n",
      " '    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n'\n",
      " '    ============== ==== ==== ======= ===== ====================\\n'\n",
      " '\\n'\n",
      " '    :Missing Attribute Values: None\\n'\n",
      " '    :Class Distribution: 33.3% for each of 3 classes.\\n'\n",
      " '    :Creator: R.A. Fisher\\n'\n",
      " '    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n'\n",
      " '    :Date: July, 1988\\n'\n",
      " '\\n'\n",
      " 'The famous Iris database, first used by Sir R.A. Fisher. The dataset is '\n",
      " 'taken\\n'\n",
      " \"from Fisher's paper. Note that it's the same as in R, but not as in the UCI\\n\"\n",
      " 'Machine Learning Repository, which has two wrong data points.\\n'\n",
      " '\\n'\n",
      " 'This is perhaps the best known database to be found in the\\n'\n",
      " \"pattern recognition literature.  Fisher's paper is a classic in the field \"\n",
      " 'and\\n'\n",
      " 'is referenced frequently to this day.  (See Duda & Hart, for example.)  The\\n'\n",
      " 'data set contains 3 classes of 50 instances each, where each class refers to '\n",
      " 'a\\n'\n",
      " 'type of iris plant.  One class is linearly separable from the other 2; the\\n'\n",
      " 'latter are NOT linearly separable from each other.\\n'\n",
      " '\\n'\n",
      " '.. topic:: References\\n'\n",
      " '\\n'\n",
      " '   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n'\n",
      " '     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n'\n",
      " '     Mathematical Statistics\" (John Wiley, NY, 1950).\\n'\n",
      " '   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene '\n",
      " 'Analysis.\\n'\n",
      " '     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n'\n",
      " '   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n'\n",
      " '     Structure and Classification Rule for Recognition in Partially Exposed\\n'\n",
      " '     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n'\n",
      " '     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n'\n",
      " '   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE '\n",
      " 'Transactions\\n'\n",
      " '     on Information Theory, May 1972, 431-433.\\n'\n",
      " '   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n'\n",
      " '     conceptual clustering system finds 3 classes in the data.\\n'\n",
      " '   - Many, many more ...')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Display the value of the DESCR key.\n",
    "pprint(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%# Examine the values of the remaining keys and determine the keys that store the values of features and class labels.# Examine the values of the remaining keys and determine the keys that store the values of features and class labels.# Examine the values of the remaining keys and determine the keys that store the values of features and class labels.\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 150  number of features: 4  number of classes: 3\n"
     ]
    }
   ],
   "source": [
    "# Examine the values of the remaining keys and determine the keys that store the values of features and class labels.\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "print(\"Dataset size: %d  number of features: %d  number of classes: %d\"\n",
    "      % (X.shape[0], X.shape[1], len(np.unique(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data set into training and test samples in the ratio of 70/30.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.0001, 'average': False, 'class_weight': None, 'early_stopping': False, 'epsilon': 0.1, 'eta0': 0.0, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': None, 'penalty': 'l2', 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# Create a linear classification model SGDClassifier and list the available model parameters.\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "def list_sgd_parameters():\n",
    "    model = SGDClassifier()\n",
    "    print(model.get_params())\n",
    "\n",
    "\n",
    "list_sgd_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.000325, 'loss': 'squared_hinge', 'n_iter_no_change': 7, 'penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "# Create a parameter grid that includes 4 types of loss function, two types of regularizers, 5 values of the regularization coefficient from 0.0001 to 0.001, and the number of iterations from 5 to 10 with a step of 1.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def grid_search_sgd():\n",
    "    model = SGDClassifier()\n",
    "    param_grid = {'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "                  'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "                  'alpha': np.linspace(0.0001, 0.001, 5),\n",
    "                  'n_iter_no_change': np.arange(5, 10, 1)}\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "\n",
    "grid_search_sgd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent: 2.16 seconds\n",
      "SGDClassifier(alpha=0.0007750000000000001, loss='perceptron',\n",
      "              n_iter_no_change=9, penalty='l1')\n",
      "{'alpha': 0.0007750000000000001,\n",
      " 'loss': 'perceptron',\n",
      " 'n_iter_no_change': 9,\n",
      " 'penalty': 'l1'}\n",
      "0.9714285714285715\n"
     ]
    }
   ],
   "source": [
    "# Create a GridSearchCV object, pass it the previously created classifier and parameter grid, and train it. Use accuracy as a metric. Provide for the output of the time spent on iterating over the grid.\n",
    "\n",
    "def grid_search_sgd_time():\n",
    "    model = SGDClassifier()\n",
    "    param_grid = {'loss': ['hinge', 'perceptron'],\n",
    "                  'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "                  'alpha': np.linspace(0.0001, 0.001, 5),\n",
    "                  'n_iter_no_change': np.arange(5, 10, 1)}\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "    start = time.time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    print(\"Time spent: %.2f seconds\" % (end - start))\n",
    "    pprint(grid_search.best_estimator_)\n",
    "    pprint(grid_search.best_params_)\n",
    "    pprint(grid_search.best_score_)\n",
    "\n",
    "\n",
    "grid_search_sgd_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent: 0.19 seconds\n",
      "SGDClassifier(alpha=0.001, loss='perceptron', n_iter_no_change=9, penalty='l1')\n",
      "{'alpha': 0.001, 'loss': 'perceptron', 'n_iter_no_change': 9, 'penalty': 'l1'}\n",
      "0.9619047619047618\n"
     ]
    }
   ],
   "source": [
    "# Organize a random grid search. Print the time spent on random enumeration.\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "def random_search_sgd_time():\n",
    "    model = SGDClassifier()\n",
    "    param_grid = {'loss': ['hinge', 'perceptron'],\n",
    "                  'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "                  'alpha': np.linspace(0.0001, 0.001, 5),\n",
    "                  'n_iter_no_change': np.arange(5, 10, 1)}\n",
    "    random_search = RandomizedSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "    start = time.time()\n",
    "    random_search.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    print(\"Time spent: %.2f seconds\" % (end - start))\n",
    "    pprint(random_search.best_estimator_)\n",
    "    pprint(random_search.best_params_)\n",
    "    pprint(random_search.best_score_)\n",
    "\n",
    "\n",
    "random_search_sgd_time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}